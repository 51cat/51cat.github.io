---
title: 多元线性回归
categories: [Bioinfo, 机器学习导论笔记]
tags: [ml]     # TAG names should always be lowercaseyi
---

在实际中还会存在多个变量的情况，

- 一种建模方法是建立多个简单回归模型。这种方法每次构建模型**都会忽略其他变量的影响，结果可能具有误导性**
- 更好的方法是扩展简单线性回归模型 ，使其可以直接包含多个预测变量

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683528779222-b9cf93b7-75c5-4c69-b3cb-8c2e36695927.png" alt="image.png" style="zoom:67%;" />

### 系数估计

多元线性回归中的参数也是用最小二乘法进行估计，使残差平方和最小，RSS计算方法与简单回归一致

例如下面的例子：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683529045145-a617fe2d-4c55-4d7e-bbbf-78af9c4a5e5d.png" alt="image.png" style="zoom:67%;" />

简单回归中的斜率表示在忽略其他预测变量(如 TV 和 radio) 的情况下，报纸广告费用增加一千美元的平均效果。

在多元回归模型中，**newspaper 的系数表示在 TV 和 radio 保持不变的情况下，报纸广告费用增加 1 千美元所产生的平均效果。**

同样可以通过t检验来得到每个系数的p-value，计算方法与简单回归一致，例如

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683529193014-ab1608aa-f90f-45db-8afa-301c9b412908.png" alt="img" style="zoom:67%;" />

newspaper 的p-value较高，相关分析也发现newspaper和radio的相关性较高，newspaper 是 radio 广告对销量的影响的一个替代品，**newspaper 通过 radio 对 sales 的影响来获得"认可"。**拟合的时候可以去掉这个变量

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683529230264-b8528908-b70e-45a5-a317-e666d926a3b2.png" alt="image.png" style="zoom:67%;" />

### 系数准确性

1. 在X1 X2 ... Xn中是否至少有一个可以预测响应变量
2. 所有预测变量都能解释Y吗
3. 拟合程度如何衡量
4. 准确度如何衡量

在简单的线性回归中，为确定响应变量和预测变量是否相关，我们可以简单地检验 β1 是否为 0 。在有 p 个预测变量的多元回归模型中，我们要问的是所有的回归系数是否均为零，在多数情况下，看的是系数的规模为 q 的特定子集是否为零。这一检验问题所对应的零假设是:

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20231122104511439.png" alt="image-20231122104511439" style="zoom:67%;" />

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683530004362-4ebca954-1340-4929-beae-59101e87cb73.png" alt="image.png" style="zoom:67%;" />

为什么要F检验：

![image.png](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683530209276-ac04fada-0c2c-4233-be44-4bcccd7430c8.png)

### 变量选择

- **前向选择**：**我们从零模型开始，然后建立简单线性回归模型，**并把使 RSS 最小的变量添加到零模型中**。然后再**加入一个新变量，得到新的双变量模型，加人的变量是使新模型的 RSS 最小的变量**。这一过程持续到满足某种停止规则为止。
- **向后选择：**我们先从包含所有变量的模型开始，**并删除 p 值最大的变量统计学上最不显著的变量**。拟合完包含 (p -1)个变量的新模型后，再删除 p 值最大的变量。此过程持续到满足某种停止规则为止。p>n时不能用
- **混合选择：**与向前选择相同，我们从不含变量的模型开始，向模型中添加拟合最好的变量，然后依次增加变量。**一旦模型中的某个变量的 p 值超过一定的阈值时，就从模型中删除该变量。**我们不断执行这些向前或向后的步骤，直到模型中所有变量的 p值都足够低，且模型外的任何变量加入模型后都将有较大的 p 值。p>n时不能用

### 拟合效果

在简单回归中，R2是响应变量和预测变量的相关系数的平方, 多元线性回归中，是响应值和线性模型拟合值的相关系数的平方

### 扩展

标准线性回归的最重要两个假设为：

- 可加
- 线性

可加：预测变量的响应变量和其他变量无关

线性：无论预测变量取何值其一个变化单位引起的响应变量的变化是恒定的

1. **去除可加性假设**

例如两个预测变量X1 X2, 当两者同时变化时，响应变量的变化情况与分别单独变化时不同，即协同效应，统计学中称为交互作用，可以通过扩展模型纳入交互效应，含有交互项的模型可能优于仅包含主效应的模型

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683619903800-85e004eb-7816-451c-9896-267e1848673a.png)

实验分层原则 (hierarchical principle) 规定，如果模型中含有交互项，那么即使主效应的系数的p值不显著，也应包含在模型中。

1. **非线性关系**

对于非线性关系，可以使用多项式进行拟合，例如mpg数据集中

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683620208166-608ded5d-6b95-41c1-8542-0c377ffa0223.png)

上述仍然是线性模型，其中X1 = horsepower，X2 = horsepower2

## 其他

### 数据是否非线性

可以通过残差图进行判断，

- 简单线性回归：预测变量 - 残差
- 多元线性回归：响应值 - 残差

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683620396087-cb4176ba-9007-4a9d-85a9-ca3028c9f620.png" alt="img" style="zoom:67%;" />

一般残差图应该没有明显的规律，如左图则可以看出残差图有一定的U型，说明数据为非线性，将horsepowe^2引入后，右图的残差图则没有明显区域，**如果数据存在非线性关系，可以对数据进行非线性转换，如log，平方，开根号......**

### 误差自相关

线性回归模型的一个重要假设是误差项不相关，假设两个误差a，b不相关就是完全无法通过a的值预测b的值，回归系数和拟合值的标准误的计算都基于误差项不相关的假设。如果误差项之间有相关性，那么估计标准误往往低估了真实标准误。因此，置信区间和预测区间比真实区间窄。

在时间序列的数据中，此情况较为常见，下图误差相关性越来越高，通过优化实验设计来规避

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683620923261-78eeaa93-8342-427a-a803-a9b020b9dc57.png)

### 误差方差非恒定

线性回归模型的另一个重要假设是误差项的方差是恒定的，如果残差图呈漏斗形 (funnel shape) ，说明误差项方差非恒定或存在异方差性( heteroscedasticity )，一个可能的解决方案是用**凹函数对响应值 y 做变换，如取对数，开根号等，使Y具有较小的伸缩性，降低异方差性**

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683621128868-58c8c1bb-165b-455e-9c8d-89366db6fb54.png" alt="img" style="zoom:67%;" />

前者具有异方差性，残差图呈现漏斗状

### 离群值

离群点( outlier) 是指 yi 远离模型预测值的点。产生离群点的可能原因有很多，如数据收集过程中对某个观测点的错误记录，可以绘制学生化残差估计， 学生化残差绝对值大于 3 的观测点可能是离群点

**学生**化残差 是 残差 除以它的 标准差 后得到的数值

### 高杠杆值

与离群值相反，高杠杆值是观测值x为异常的点，如下图41点为异常的，因为他的预测值比别的都要大很多，去除高杠杆点比去除离群点对最小二乘线的影响更大 

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683621586496-01a8c13f-4dad-49d7-b337-b41cdfc82a81.png)

在简单线性回归中，高杠杆观测是很容易辨认的，我们可以简单地找到预测变量的取值超出正常范围的观测点。但是，在有许多预测变量的多元线性回归中，可能存在这样的观测点: **单独来看，它各个预测变量的取值都在正常范围内，但从整个预测变量集的角度来看，它却是不寻常的**。

可以通过计算杠杆统计量衡量：

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683621708775-c9231538-3f1c-40e0-8447-969574c9817c.png)

如果给定观测的杠杆统计量大大超过 (p+1)/n ， 可能对应点有较高的杠杆作用。

### 共线性

共线性( collinearity) 是指两个或更多的预测变量高度相关在回归中，共线性的存在可能会引发问题，因为可能难以分离出单个变量对响应值的影响 

- 检测共线性的一个简单方法是看预测变量的相关系数矩阵。该矩阵中出现绝对值大的元素表示有一对变量高度相关，因此数据中存在共线性问题
- **即使没有某对变量具有特别高的相关性，有可能三个或更多变量之间存在共线性。**我们把这种情况称为多重共线性( multicollinearity) 一种更好的评估多重共线性的方式是计算**方差膨胀因子** (variance inflation factor , VIF) 

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683621883876-dac2ba5c-06d4-46cd-981f-7bb82693912e.png)

![img](https://51catgithubio.oss-cn-beijing.aliyuncs.com/1683621918114-5b40e473-1224-4057-bf6e-68e622711eb0.png)

解决：

1. 共线性问题有两种简单的解决方案。第一种是从回归中剔除一个问题变量
2. 第二种解决方案是把共线变量组合成一个单一的预测变量

### KNN回归和分类

KNN (k-nearest neighbors) 是一种基本的机器学习算法，它可以应用于分类和回归问题。 KNN 分类和 KNN 回归之间的差异：

KNN 分类：KNN 分类是指使用 KNN 算法进行分类任务。在 KNN 分类中，给定一个未知样本，算法会将其分配给最靠近它的 k 个已知样本类别中出现次数最多的类别。因此，KNN 分类是一种离散的预测算法。

KNN 回归：KNN 回归是指使用 KNN 算法进行回归任务。在 KNN 回归中，给定一个未知样本，算法会计算其 k 个最近邻样本的均值或加权平均值，并将该值作为预测结果。因此，KNN 回归是一种连续的预测算法。

主要区别：

1. 预测结果不同：KNN 分类输出的是目标类别，而 KNN 回归输出的是目标值。
2. 应用场景不同：KNN 分类适用于分类问题，KNN 回归适用于回归问题。
3. 距离度量不同：在 KNN 分类中通常使用欧氏距离、曼哈顿距离等度量方法来计算样本之间的距离，而在 KNN 回归中通常使用加权平均或均值法计算最近邻样本的距离。

KNN 算法可以应用于分类和回归等不同类型的问题，并且在应用时需要根据具体情况选择合适的距离度量方法和参数。
